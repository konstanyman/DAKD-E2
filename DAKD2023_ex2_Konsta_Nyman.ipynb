{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = red> Please fill up the asked information!\n",
    "    \n",
    "<font color = teal>Name:\n",
    "\n",
    "<font color = teal>Student number:\n",
    "\n",
    "<font color = teal>Mail:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rpio-nMAoDwq"
   },
   "source": [
    "------\n",
    "\n",
    "# Data Analysis and Knowledge Discovery: Exercise 2, Supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90c9fF6woDwr"
   },
   "source": [
    "The previous exercise was about <i>data understanding</i> and <i>data preparation</i>, which formed the basis for the modeling phase of the data mining process. Many modeling techniques make assumptions about the data, so the exploration and preparation phases can't be ignored. Now, as we have checked the validity of data and familiarized ourselves with it, we can move on to the next stage of the Cross-Industry Standard Process for Data Mining (CRISP-DM), which is <font color = green>modeling</font>.\n",
    "\n",
    "The questions to be answered at this stage could include:\n",
    "\n",
    "- What kind of model architecture best fits our data?\n",
    "- How well does the model perform technically?\n",
    "- Could we improve its performance?\n",
    "- How do we evaluate the model's performance?\n",
    "\n",
    "<i>Machine learning</i> is a subfield of artificial intelligence that provides automatic, objective and data-driven techniques for modeling the data. Its two main branches are <i>supervised learning</i> and <i>unsupervised learning</i>, and in this exercise, we are going to use the former, <font color = green>supervised learning</font>, for classification and regression tasks.\n",
    "\n",
    "For classification, data remains the same as in the previous exercise, but I've already cleaned it up for you. Some data pre-processing steps are still required to ensure that it's in an appropriate format so that models can learn from it. Even though we are not conducting any major data exploration nor data preparation this time, <i>you should <b>never</b> forget it in your future data analyses</i>.\n",
    "\n",
    "-----\n",
    "\n",
    "<b>General guidance for exercises</b>\n",
    "\n",
    "- Answer <b>all</b> questions below, even if you can't get your script to fully work.\n",
    "- Write clear and easily readable code, and include explanations what your code does.\n",
    "- Make informative illustrations: include labels for x and y axes, legends and captions for your plots.\n",
    "- You can add more code and markup cells, as long as the flow of the notebook stays readable and logical.\n",
    "- <b>A complete submission includes a working notebook</b>, so it's highly recommended to run \"Restart & Run all\" before the final save. Remember to submit BOTH versions of the exercise (ipynb AND html/pdf). \n",
    "- Grading: *Fail*/*Pass*/*Pass with honors* (+1)\n",
    "    - Passing requires that <b>the parts 1-5</b> are completed.\n",
    "    - +1 bonus point (grading *Pass with honors*) requires a <b>completely</b> correct solution and also thorough analysis.\n",
    "- If you encounter problems, Google first. Always give the credit where it belongs so please <b>cite your sources, whether you're referencing text or code</b>. You will learn so much more when you have to research and summarize information in your own words. If you can't find an answer to the problem, don't hesitate to ask in the Moodle discussion or directly via email (tuhlei@utu.fi).\n",
    "- Note! Don't leave it to the last moment! No feedback service during weekends.\n",
    "- <b>We do not encourage the use of ChatGPT or similar models</b>, but if you choose to do so, always be critical of the outputs and try to comprehend them before any use. Also, make a brief description how you utilized the model (what was your input and how did you benefit from the output).\n",
    "\n",
    "\n",
    "<font color = green> The guided exercise session is held on the 28th of November at 14:15-16:00, at TSE Elovena-Sali.</font>\n",
    "\n",
    "<font color = red size = 4><b>The deadline is the 30th of November at 23:59</b></font>. Late submissions will not be accepted unless there is a valid excuse for an extension which should be asked **before** the original deadline.\n",
    "\n",
    "\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2I2WLapM3BPc"
   },
   "source": [
    "### Gather all packages needed for this notebook here:\n",
    "\n",
    "You can use other packages as well, but this excercise can be completed with those below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "iypIAVquoDws"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization packages - matplotlib and seaborn\n",
    "# Remember that pandas is also handy and capable when it comes to plotting!\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Machine learning package - scikit-learn\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, LeaveOneOut, GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "\n",
    "# Show the plots inline in the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________\n",
    "## <font color = lightcoral>1. Classification using k-nearest neighbors </font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CqWZYx2oDw3"
   },
   "source": [
    "We start exploring the world of data modeling by using the <font color = lightcoral>K-Nearest Neightbors (k-NN) algorithm</font>. The k-NN algorithm is one of the classic supervised machine learning algorithms which assumes that similar points are close to each other. \n",
    "\n",
    "In our case, we'll use the k-NN algorithm to *predict the presence of cardiovascular disease* (CVD) using all the other variables as <font color = lightcoral>features</font> in the given data set. I.e. the <font color = lightcoral>target variable</font> that we are interested in is `cardio`.\n",
    "\n",
    "But first, we need data for the task. The code for loading the data into the environment is provided for you. The code should work but make sure that you have the CSV file of the data in the same directory where you have this notebook file.\n",
    "\n",
    "**Exercise 1 A)**\n",
    "\n",
    "Take a random sample of 1500 rows from the dataframe using your student id as a seed. Print the first 15 rows to check that everything is ok with the dataframe.\n",
    "\n",
    "*Note: as mentioned, the data remains the same, but cholesterol has been one-hot-encoded for you already. There's also a new variable, gluc (about glucose levels), which is one-hot-encoded for you. It has similar values as cholesterol originally had [normal, at risk, elevated].*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading code provided\n",
    "# ------------------------------------------------------\n",
    "# The data file should be at the same location than the \n",
    "# exercise file to make sure the following lines work!\n",
    "# Otherwise, fix the path.\n",
    "# ------------------------------------------------------\n",
    "\n",
    "# Path for the data\n",
    "data_path = 'ex2_cardio_data.csv'\n",
    "\n",
    "# Read the CSV file \n",
    "cardio_data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code - Resample and print 15 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfq_3_WNoDw3"
   },
   "source": [
    "----\n",
    "\n",
    "We have the data so now, let's put it to use. \n",
    "\n",
    "To teach the k-NN algorithm (or any other machine learning algorithm) to recognize patterns, we need <font color = lightcoral>training data</font>. However, to assess how well a model has learned these patterns, we require <font color = lightcoral>test data</font> which is new and unseen by the trained model. It's important to note that the test set is not revealed to the model until after the training is complete.\n",
    "\n",
    "So, to *estimate the performance of a model*, we may use a basic <font color = lightcoral>train-test split</font>. The term \"split\" is there because we literally split the data into two sets.\n",
    "\n",
    "**Exercise 1 B)**\n",
    "\n",
    "Collect the features as an array named `features`, and the target variable as an array named `labels`. Create training and test data by randomly splitting the data into training (80%) and test (20%) sets.\n",
    "\n",
    "- Do you need stratification for our dataset? Explain your decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Code - Train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = lightcoral> \\<Write your answer here\\></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "\n",
    "**Exercise 1 C)** \n",
    "\n",
    "Standardize the numerical features: Note that you should now have two separate features that you've divided all the features into.\n",
    "\n",
    "- Describe how the k-NN model would make predictions about whether or not a patient has a CVD when the features are not standardized, and explain the reasons behind.\n",
    "\n",
    "\n",
    "*Note: Some good information about preprocessing and how to use it for train and test data can be found https://scikit-learn.org/stable/modules/preprocessing.html*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code - Standardization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = lightcoral> \\<Write your answer here\\></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "muqCazPIoDw4"
   },
   "source": [
    "-------\n",
    "\n",
    "It's time for us to train the model!\n",
    "\n",
    "**Exercise 1 D)**\n",
    "\n",
    "Train a k-NN model with $k=3$. Print out the confusion matrix and use it to compute the accuracy, the precision and the recall.\n",
    "- What does each cell in the confusion matrix represents in the context of our dataset?\n",
    "- How does the model perform with the different classes? Where do you think the differences come from? Interpret the performance metrics you just computed.\n",
    "- With our dataset, why should you be a little more cautious when interpreting the accuracy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "NMR7Y2s6oDw4",
    "outputId": "33bd42f3-a25c-47de-cb12-908d698d08af"
   },
   "outputs": [],
   "source": [
    "### Code - the kNN classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = lightcoral> \\<Write your answer here\\></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "\n",
    "You randomly divided the data into two sets, one for training the k-NN model and the other for evaluating its performance. However, randomness is not the thing we really need, and in fact, it's not something we even desire. Instead, what we do want is to keep track of each step we're making and exporing. This said, the *reproducibility* of the results is extremely important in research. To achieve this, we can utilize <font color = lightcoral> a random seed</font>, with which we can re-run the codes and get the exact same results than before.\n",
    "\n",
    "For example, we can use a fixed seed when we're shuffling the data before splitting it into training and test sets. This ensures that when we're re-runing the code, we obtain exactly the same partitions of the data in each split.\n",
    "\n",
    "**Exercise 1 E)**\n",
    "\n",
    "Initialize 1000 random seeds and continue with the k-NN model ($k=3$): Perform 1000 different train-test splits using these seeds and store the accuracies from each split. Plot the accuracies in a histogram, and discuss your results.\n",
    "\n",
    "*Tip: You can add the accuracy from the previous exercise in the plot by drawing a vertical line with the function `matplotlib.axes.Axes.axvline(<accuracy>)` if you want!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code - Initialization of the 1000 fixed seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code - 1000 different train-test-splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = lightcoral> \\<Write your answer here\\></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "One really common evaluation metric is <font color = lightcoral>the area under the receiver operating characteristic (AUROC, AUC or AUC-ROC)</font>.  It provides a measure of a model's ability to distinguish between classes, especially in binary classification tasks between the <i>positive class</i> and the <i>negative class</i>. (There sure are multiclass and multilabel cases too, but they are out of scope here.) \n",
    "\n",
    "In our case, individuals who have a CVD form the positive class. As the name of the measure suggests, it combines two keys aspects to interpret a model's performance: ROC curves are about the trade-off between the true positive rate and the false positive rate, the former representing the model's ability to correctly identify individuals with a CVD (true positives) and the latter measuring the model's tendency to incorrectly classify individuals without a CVD as if they have the disease (false positives). Thus, the area beneath the curve is simply the AUROC, a single numerical value, that summarizes the overall performance.\n",
    "\n",
    "**Exercise 1 F)** \n",
    "\n",
    "Evaluate the performance of the trained k-NN model by computing the AUROC and plotting the related curve. Draw also the line for random guesses.\n",
    "\n",
    "- How well does the k-NN model perform in distinguishing between healthy individuals and those with a CVD?\n",
    "\n",
    "*Tip: You should not use the predicted labels in this exercise.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code - ROC curve and AUROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = lightcoral> \\<Write your answer here\\></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQfehqAioDw4"
   },
   "source": [
    "__________\n",
    "## <font color = royalblue> 2. Classification accuracy using leave-one-out cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPZx-6JLoDw5"
   },
   "source": [
    "While the train-test split may provide us with an unbiased estimate of the performance, we only evaluate the model once. Especially when dealing with small datasets, a test set itself will be very small. How can we be sure that the evaluation is accurate with this small test set and not just a good (or bad) luck? And what if we'd like to compare two models and the other seems to be better -- how can we be sure that it's not just a coincidence?\n",
    "\n",
    "Well, there's a great help available and it's called <font color = royalblue>cross-validation</font>. With its help, we can split the dataset into multiple different training and test sets, which allows us to evaluate models across various data partitions. This time, we'll take a closer look at the <font color = royalblue>leave-one-out cross-validation</font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**\n",
    "\n",
    "Let's keep the focus on detecting the CVD, so once again we utilize the k-NN model (with $k=3$) to predict the precense of the disease. Now, apply leave-one-out cross-validation to assess whether the k-NN model is suitable for addressing the problem. You may use the entire dataset on this task.\n",
    "\n",
    "- What can you say about the accuracy compared to the previous task?\n",
    "- What do you think: Does the k-NN model work for the problem in hand? Explain your answer.\n",
    "\n",
    "*Tip: This sure can be done manually, but `cross_val_score` is quite a handy function too.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "znHjVwKDoDw5",
    "outputId": "3b73cc4a-eef7-4e9e-be17-19d79df71033",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Code - Leave-one-out cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = royalblue> \\<Write your answer here\\></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "88BjCQL6oDw5"
   },
   "source": [
    "____________\n",
    "## <font color = forestgreen> 3. Model selection with leave-one-out cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A8fcES_LoDw5"
   },
   "source": [
    "So far, we've trained one model at a time and I've given the value of k for you. Accuracy is what it is (no spoilers here), but could we still do a little better? Let's explore that possibility through a process known as <font color=green>hyperparameter tuning</font>. The cross-validation is especially important tool for this task. Note here, that model selection and model evaluation (or assessment) are two different things: We use model selection to estimate the performance of various models to identify the model which is most likely to provide the \"best\" predictive performance for the task. And when we have found this most suitable model, we *assess* its perfomance and generalisation power on unseen data.\n",
    "\n",
    "This time, we're going to train multiple models, let's say 30, and our goal is to select the best K-Nearest Neighbors model from this set. Most models come with various hyperparameters that require careful selection, and the k-NN model is no exception. Although we're talking about the number of neighbors here, it's important to note that k-NN also has several other hyperparameters. However, for the sake of simplicity, this time we'll focus solely on fine-tuning the number of nearest neighbors, that is, the value of k. \n",
    "\n",
    "Let's focus on the model selection part here for the sake of comprehending the cross-validation itself. We'll get later on the whole pipeline, which also includes model assessment.\n",
    "\n",
    "**Exercise 3**\n",
    "\n",
    "Find the optimal k value from a set of $k=1...30$ using leave-one-out cross-validation. Plot the accuracies vs. the k values. Again, you may use the entire dataset.\n",
    "\n",
    "- Which value of k produces the best accuracy when using leave-one-out cross-validation? Compare the result to the previous model with $k=3$.\n",
    "- If the number of k is still increased, what is the limit that the accuracy approaches? Why?\n",
    "- Discuss the impact of choosing a very small or very large number of neighbors on the k-NN model's ability to distinguish between the healthy individuals and the ones with CVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RJ7570_ZoDw6",
    "outputId": "91caaf74-2636-46f7-a4e3-4e5ba98e4c4e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Code - Select best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code - Plot the accuracies vs. the values for k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = green>\\<Write your answer here\\></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9r2v1LEDoDw6"
   },
   "source": [
    "________________\n",
    "## <font color = red>  4. Training and testing on the same dataset </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j6B6L5HWoDw6"
   },
   "source": [
    "<i>Note that this should never be done outside of this exercise! You have been warned.</i>\n",
    "\n",
    "Oh, but what if we just trained a model using the *entire* dataset? Wouldn't we like to use as much data as possible to discover the underlying patterns in the data so why not to use all of it?\n",
    "\n",
    "**Exercise 4**\n",
    "\n",
    "This is quite straightforward: Train 30 k-NN models ($k = 1...30$ ) using the whole dataset and evaluate the trained models using, again, the whole dataset. Create a plot that displays the accuracies against the corresponding k values. Include the accuracy values from the previous task in the same figure.\n",
    "\n",
    "- What's the optimal value for k now and why's that? How would you interpret the reliability of the predictions?\n",
    "- Explain why you should never use the same dataset for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ExWjmjQ5oDw6",
    "outputId": "f7bd8eac-a636-400b-a8a4-9ca269129628",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Code - Train with whole data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code - Plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = red>\\<Write your answer here\\></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________\n",
    "\n",
    "## <font color = darkorange> 5. Comparison of ridge regression and k-NN regression </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous exercises were about classification. Now, we are ready to see another kind of supervised learning - regression - as we are changing our main goal from predicting discrete classes (healty/sick) to estimating continuous values. The following exercises are going to involve utilizing two different regression models, <font color = darkorange>Ridge Regression</font> and <font color = darkorange>K-Nearest Neighbors (k-NN) Regression</font>, and our goal is to evaluate and compare the performances of these two models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change the dataset to make the following exercises more intuitive. The new dataset is about brushtail possums and it includes variables such as\n",
    "\n",
    "- <b>sex</b>: Gender, either male (0) or female (1)\n",
    "- <b>age</b>: Age in years\n",
    "- <b>len_head</b>: Head length in cm\n",
    "- <b>width_skull</b>: Skull width in mm\n",
    "- <b>len_total</b>: Total length in cm\n",
    "- <b>len_tail</b>: Tail length in cm\n",
    "- <b>len_foot</b>: Foot length \n",
    "- <b>len_earconch</b>: Ear conch length \n",
    "- <b>width_eye</b>: Distance from medial canthus to lateral canthus of right eye, i.e., eye width\n",
    "- <b>chest</b>: Chest grit in cm\n",
    "- <b>belly</b>: Belly grit in cm\n",
    "\n",
    "In this case, our target variable will be *the age of the possum*. The data for this exercise has been modified from the original source.\n",
    "\n",
    "There's the code chunk for loading data provided again. <font color = red>Again, the data file should be located in the same directory as this notebook file!</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading code provided\n",
    "# ------------------------------------------------------\n",
    "# The data file should be at the same location than the \n",
    "# exercise file to make sure the following lines work!\n",
    "# Otherwise, fix the path.\n",
    "# ------------------------------------------------------\n",
    "\n",
    "# Data path\n",
    "data_path = 'ex2_possum_data.csv'\n",
    "\n",
    "# Load the data \n",
    "possum_data = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------\n",
    "\n",
    "Regression allows us to examine <font color = darkorange>relationships between two or more variables</font>. This relationship is represented by an *equation*, which itself represents how a change in one variable affects another on average. For example, we could examine how a change in possum's total length affects, on average, its estimated age.\n",
    "\n",
    "We start by examing those relationships between the variables in the given dataset.\n",
    "\n",
    "\n",
    "**Exercise 5 A)**\n",
    "\n",
    "Plot pairwise relationships between the age variable and the others where you color the samples based on the sex variable. \n",
    "\n",
    "- Which body dimensions seem to be most correlated with age? And are there any variables that seem to have no correlation with it?\n",
    "- Are there any differences in the correlations between males and females?\n",
    "\n",
    "*Tip: `seaborn.pairplot()` is handy with the parameters `(x,y)_vars` and `hue`. You actually can fit a linear model to draw a regression line with the parameter `kind` set to `\"reg\"`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code - Pairplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = darkorange>\\<Write your answer here\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "Before the regression analysis itself, let's check that our dataset is in a proper format. We'll also perform the train-test split as we're going to first tune the hyperparameters for each model using the training set and test the overall performance of the chosen models using the test set.\n",
    "\n",
    "**Exercise 5 B)**\n",
    "\n",
    "Do you need to prepare the data a little? Explain your decision. Perform the train-test (80/20) split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code - Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = darkorange>\\<Write your answer here\\></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "Regarding the k-NN, we have already get familiar with the optimization of the k value. The idea behind the k-NN is the same as previously in classification, the output isn't a class anymore but a continuous value. So, for now, we can stick to the optimal k value. However, for Ridge Regression, we'll focus on the hyperparameter called $\\lambda$ (read as 'lambda'), the regularization term (or penalty term or L2 penalty, how ever we'd like to call it), and try to find its optimal value for this task. After the model selection for both regression is performed, we compare the chosen models using a metric called <font color = darkorange>mean absolute error (MAE)</font>.\n",
    "\n",
    "**Exercise 5 C)**\n",
    "\n",
    "Train multiple ridge regression models and k-NN regression models. For hyperparameters, use $\\lambda=2^{-10}...2^{10}$ and $k=1...30$. Once again, use leave-one-out cross validation. Remember to use only the training dataset for model selection. Plot the optimal k values and lambdas versus corresponding MAEs (two different plots).\n",
    "\n",
    "- Introduce the optimal hyperparameters for each regression model.\n",
    "- How do you interpret the MAE in our case when the target variable is age?\n",
    "\n",
    "*Note: In a `sklearn.linear_model.Ridge` class, lambda is called as \"alpha\" so don't get confused.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Code - CV for Ridge regression and k-NN regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = darkorange>\\<Write your answer here\\></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "The cross-validation was employed for model selection and at this point, we have the optimal hyperparameter settings for each model. Let's finally assess both models using the test set. To continue from this, we first fit the chosen models using the entire training, ensuring that the models are trained with the maximum available data. \n",
    "\n",
    "**Exercise 5 D)**\n",
    "\n",
    "Fit the chosen models with the whole training set. Evaluate the models using the test set and describe the results.\n",
    "\n",
    "- How well did the models perform in estimating the possums' ages?\n",
    "- So, what's the pitfall in here if you had compared the overall performance of the models based on the cross-validation MAEs and not the MAEs for the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code - Evaluating the selected models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = darkorange>\\<Write your answer here\\></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "________________\n",
    "## <font color = slategrey> BONUS: Feature selection - most useful features in predicting cardiovascular diseases </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can stop here and get the \"pass\" grade! To get the \"pass with honors\" grade, you need to do the following exercise. This means you'll get one bonus point for the exam.\n",
    "\n",
    "The exercise is not as straightforward as the previous ones, and may require you to do some research of your own. You are also required to **explain** the steps you choose with your own words, and show that you tried to understand the idea behind the task. There's no single correct solution for this so just explain what you did and WHY you did it.\n",
    "\n",
    "----------------\n",
    "\n",
    "\n",
    "Unfortunately, due to the lack of resources and time, doctors can't measure all the values represented in the given cardio dataset. Fortunately, eager students are willing to help: Your task is to identify <font color = slategrey>four [4] most useful features</font> for predicting the presence of the CVD from the dataset. The steps needed for this job are presented above except the feature selection part. You must remember not to leak any information from the test set when selecting the features, i.e., you try to find those five features using only the training set.\n",
    "\n",
    "Regarding the feature selection itself, you may choose your weapon from the three categories: filter methods, embedded methods or wrapper methods. Use <font color = slategrey>two methods from two different category and compare their outputs</font>. Bonus material about the methods is provided in Moodle.\n",
    "\n",
    "Evaluate the model of your choice with the limited dataset by examining the confusion matrix and computing the accuracy and the AUROC.\n",
    "\n",
    "**Discuss** your findings and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code - Bonus task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color = slategrey>\\<Write your answer here\\></font>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "DADK2020_ex3_Valtteri_Nieminen.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "180px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
